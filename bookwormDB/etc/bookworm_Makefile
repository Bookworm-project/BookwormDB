#invoke with any of these variable: eg, `make`



# The maximum size of each input block for parallel processing.
# 100M should be appropriate for a machine with 8-16GB of memory: if you're
# having problems on a smaller machine, try bringing it down.

blockSize:=100M

# By default, we read full text rather than feature counts.
maybe_feature_counts=

ifeq ($(maybe_feature_counts),--feature-counts)
optional_args=$(maybe_feature_counts) -l $(logLevel) -d $(database)
else
optional_args=-l $(logLevel) -d $(database)
endif
textStream:=bookworm $(optional_args) tokenize text_stream

#logLevel can be set here.
logLevel="WARNING"

#New syntax requires bash
SHELL:=/bin/bash

#You can manually specify a bookworm name, but normally it just greps it out of your configuration file.
database:=$(shell grep database bookworm.cnf | sed 's/.* = //g')

#The data format may vary depending on how the raw files are stored. The easiest way is to simply pipe out the contents from input.txt: but any other method that produces the same format (a python script that unzips from a directory with an arcane structure, say) is just as good.
#The important thing, I think, is that it not insert EOF markers into the middle of your stream.

webDirectory=/var/www/
webSite = $(addsuffix database,webDirectory)

all: bookworm.cnf .bookworm/targets .bookworm/targets/database

bookworm.cnf:
	bookworm $(optional_args) prep configuration

#These are all directories that need to be in place for the other scripts to work properly
.bookworm/targets: 
#"-building needed directories"
	@mkdir -p .bookworm/texts
	@mkdir -p .bookworm/texts/encoded/{unigrams,bigrams,trigrams,completed}
	@mkdir -p .bookworm/texts/{textids,wordlist}
	@mkdir -p .bookworm/targets

# A "make clean" removes most things created by the bookworm,
# but keeps the database and the registry of text and wordids

clean:
#Remove inputs.txt if it's a pipe.
	find .bookworm/texts -maxdepth 1 -type p -delete
	rm -rf .bookworm/texts/encoded/*/*
	rm -rf .bookworm/targets
	rm -f .bookworm/metadata/catalog.txt
	rm -f .bookworm/metadata/jsoncatalog_derived.txt
	rm -f .bookworm/metadata/field_descriptions_derived.json

# Make 'pristine' is a little more aggressive
# This can be dangerous, but lets you really wipe the slate.

pristine: clean
	-mysql -e "DROP DATABASE $(database)"
	rm -rf .bookworm/texts/textids
	rm -rf .bookworm/texts/wordlist/*

# The wordlist is an encoding scheme for words: it tokenizes in parallel, and should
# intelligently update an exist vocabulary where necessary. It takes about half the time
# just to build this: any way to speed it up is a huge deal.
# The easiest thing to do, of course, is simply use an Ngrams or other wordlist.

# The build method is dependent on whether we're using an accumulated wordcount list
# from elsewhere. If so, we use Peter Organisciak's fast_featurecounter.sh on that, instead.

ifeq ($(maybe_feature_counts),--feature-counts)

#wordlistBuilder=.bookworm/scripts/fast_featurecounter.sh ../unigrams.txt /tmp $(blockSize) .bookworm/texts/wordlist/sorted.txt; head -1000000 .bookworm/texts/wordlist/sorted.txt > .bookworm/texts/wordlist/wordlist.txt
wordlistBuilder=cat unigrams.txt | bookworm $(optional_args) tokenize word_db
else
wordlistBuilder=$(textStream) | parallel --block-size $(blockSize) --pipe bookworm $(optional_args) tokenize token_stream | bookworm $(optional_args) tokenize word_db
endif

$(warning $(wordlistBuilder))

.bookworm/texts/wordlist/wordlist.txt:
	$(wordlistBuilder)

# This invokes OneClick on the metadata file to create a more useful internal version
# (with parsed dates) and to create a lookup file for textids in .bookworm/texts/textids

.bookworm/metadata/jsoncatalog_derived.txt: .bookworm/metadata/jsoncatalog.txt .bookworm/metadata/field_descriptions.json
#Run through parallel as well.
	cat .bookworm/metadata/jsoncatalog.txt | parallel --pipe bookworm $(optional_args) prep catalog_metadata > $@


# In addition to building files for ingest.

.bookworm/metadata/catalog.txt:
	bookworm $(optional_args) prep preDatabaseMetadata

# This is the penultimate step: creating a bunch of tsv files 
# (one for each binary blob) with 3-byte integers for the text
# and word IDs that MySQL can slurp right up.

# This could be modified to take less space/be faster by using named pipes instead
# of pre-built files inside the .bookworm/targets/encoded files: it might require having
# hundreds of blocked processes simultaneously, though, so I'm putting that off for now.

# The tokenization script dispatches a bunch of parallel processes to bookworm/tokenizer.py,
# each of which saves a binary file. The cat stage at the beginning here could be modified to 
# check against some list that tracks which texts we have already encoded to allow additions to existing 
# bookworms to not require a complete rebuild.


#Use an alternate method to ingest feature counts if the file is defined immediately below.

ifeq ($(maybe_feature_counts),--feature-counts)
encoder=cat unigrams.txt | parallel --block-size $(blockSize) -u --pipe bookworm $(optional_args) tokenize encode
else
encoder=$(textStream) | parallel --block-size $(blockSize) -u --pipe bookworm $(optional_args) tokenize encode
endif

$(warning $(encoder))

this_makefile := $(lastword $(MAKEFILE_LIST))

.bookworm/targets/encoded: .bookworm/texts/wordlist/wordlist.txt
#builds up the encoded lists that don't exist yet.
#I "Make" the catalog files rather than declaring dependency so that changes to 
#the catalog don't trigger a db rebuild automatically.
	make -f $(this_makefile) .bookworm/metadata/jsoncatalog_derived.txt
	make -f $(this_makefile) .bookworm/texts/textids.dbm
	make -f $(this_makefile) .bookworm/metadata/catalog.txt
	$(encoder)
	touch .bookworm/targets/encoded

# The database is the last piece to be built: this invocation of OneClick.py
# uses the encoded files already written to disk, and loads them into a database.
# It also throws out a few other useful files at the end into .bookworm/

.bookworm/targets/database: .bookworm/targets/database_wordcounts .bookworm/targets/database_metadata 
	touch $@

.bookworm/texts/textids.dbm: .bookworm/texts/textids .bookworm/metadata/jsoncatalog_derived.txt .bookworm/metadata/catalog.txt
	bookworm -l $(logLevel) -d $(database) prep text_id_database

.bookworm/targets/database_metadata: .bookworm/targets/encoded .bookworm/texts/wordlist/wordlist.txt .bookworm/targets/database_wordcounts .bookworm/metadata/jsoncatalog_derived.txt .bookworm/metadata/catalog.txt 
	bookworm -l $(logLevel) -d $(database) prep database_metadata
	touch $@

.bookworm/targets/database_wordcounts: .bookworm/targets/encoded .bookworm/texts/wordlist/wordlist.txt
	bookworm -l $(logLevel) -d $(database) prep database_wordcounts
	touch $@

# the bookworm json is created as a sideeffect of the database creation: this just makes that explicit for the webdirectory target.
# I haven't yet gotten Make to properly just handle the shuffling around: maybe a python script inside "etc" would do better.

$(webDirectory)/$(database):
	git clone https://github.com/Bookworm-project/BookwormGUI $@

linechartGUI: $(webDirectory)/$(database) .bookworm/$(database).json
	cp .bookworm/$(database).json $</static/options.json

### Some defaults to make it easier to clone this directory in:



.bookworm/metadata/jsoncatalog.txt:
# First look for the file in the current directory: then in one directory below.
	@mkdir -p .bookworm/metadata
	@if [ -f ../jsoncatalog.txt ]; then \
		ln -sf ../../../jsoncatalog.txt $@; \
	fi
	@if [ -f jsoncatalog.txt ]; then \
		ln -sf ../../jsoncatalog.txt $@; \
	fi



.bookworm/metadata/field_descriptions.json:
	@mkdir -p .bookworm/metadata
	@if [ -f ../field_descriptions.json ]; then \
		ln -sf ../../../field_descriptions.json $@; \
	fi
	@if [ -f field_descriptions.json ]; then \
		ln -sf ../../field_descriptions.json $@; \
	fi

